// import SparkMD5 from './spark-md5.min.js'
// ç”±äºåŒæºç­–ç•¥åªèƒ½åŠ è½½åŒæºæ–‡ä»¶
self.importScripts('spark-md5.min.js')

console.log(`worker is working!!!`)

// æ–‡ä»¶ä½¿ç”¨å¢é‡è®¡ç®—çš„é˜ˆå€¼å¤§å°
const fileIncrementalMdSize = 5 * 1024 * 1024

// åˆ›å»ºæ–‡ä»¶åˆ‡ç‰‡
function createFileChunk(file, chunkSize) {
    return new Promise((resolve, reject) => {
        let fileChunkList = []
        let cur = 0
        console.group()
        while (cur < file.size) {
            // Blob æ¥å£çš„ slice() æ–¹æ³•åˆ›å»ºå¹¶è¿”å›ä¸€ä¸ªæ–°çš„ Blob å¯¹è±¡ï¼Œè¯¥å¯¹è±¡åŒ…å«è°ƒç”¨å®ƒçš„ blob çš„å­é›†ä¸­çš„æ•°æ®ã€‚
            fileChunkList.push({ chunkFile: file.slice(cur, cur + chunkSize) })
            cur += chunkSize

            console.log(
                `å½“å‰åˆ‡å‰²ä½ç½®ç›¸å¯¹æ•´ä¸ªæ–‡ä»¶çš„å¤§å°${cur / 1024 / 1024}MBï¼Œæ€»è®¡${
                    file.size / 1024 / 1024
                }MB`,
            )
        }
        console.groupEnd()
        // è¿”å›å…¨éƒ¨æ–‡ä»¶åˆ‡ç‰‡
        resolve(fileChunkList)
    })
}

// è¿™é‡Œå®ç°ä¸€ä¸ªå…¨é‡çš„MD5è®¡ç®—ï¼Œä»…é’ˆå¯¹å°æ–‡ä»¶
const calculateChunksHashMin = async (file, fileChunkList) => {
    try {
        const spark = new SparkMD5.ArrayBuffer()

        const reader = new FileReader()
        reader.readAsArrayBuffer(file)
        reader.onload = (e) => {
            spark.append(e.target.result)
        }
        reader.onerror = (err) => {
            reject(err) // å¦‚æœè¯»å–é”™è¯¯ï¼Œåˆ™æ‹’ç»Promise
        }

        const md5 = spark.end()
        console.log(`å…¨é‡æ–‡ä»¶MD5${md5}`)

        self.postMessage({ percentage: 100, fileHash: md5, fileChunkList }) // å‘é€æœ€ç»ˆç»“æœåˆ°ä¸»çº¿ç¨‹
        self.close() // å…³é—­Worker
    } catch (err) {
        self.postMessage({ name: 'error', data: err }) // å‘é€é”™è¯¯åˆ°ä¸»çº¿ç¨‹
        self.close() // å…³é—­Worker
    }
}

// åŠ è½½å¹¶è®¡ç®—æ–‡ä»¶åˆ‡ç‰‡çš„MD5
async function calculateChunksHash(fileChunkList) {
    // åˆå§‹åŒ–è„šæœ¬
    // const spark = new SparkMD5.ArrayBuffer()
    const spark = new SparkMD5.ArrayBuffer()

    // è®¡ç®—åˆ‡ç‰‡è¿›åº¦ï¼ˆæ‹“å±•åŠŸèƒ½ï¼Œå¯è‡ªè¡Œæ·»åŠ ï¼‰
    let percentage = 0

    // è®¡ç®—åˆ‡ç‰‡æ¬¡æ•°
    let count = 0

    // è¿™é‡Œçš„å«ä¹‰æ˜¯ï¼Œå°†åˆ†ç‰‡çš„æ–‡ä»¶ä¾æ¬¡åŠ å…¥sparkä¹‹åè®¡ç®—hashå€¼ï¼Ÿé‚£æˆ‘ä¸ºå•¥ä¸ç›´æ¥è®¡ç®—æ•´ä¸ªæ–‡ä»¶çš„ï¼Œé€’å½’ä»€ä¹ˆå‘¢
    // https://zqd123.github.io/js-spark-md5-testpage/test/test.html å› ä¸ºä½¿ç”¨å…¨é‡çš„è®¡ç®—æ–¹å¼æœ‰å¯èƒ½ä¼šå´©æºƒ
    // é€’å½’å‡½æ•°ï¼Œç”¨äºå¤„ç†æ–‡ä»¶åˆ‡ç‰‡
    async function loadNext(index) {
        if (index >= fileChunkList.length) {
            // æ‰€æœ‰åˆ‡ç‰‡éƒ½å·²å¤„ç†å®Œæ¯•
            return spark.end() // è¿”å›æœ€ç»ˆçš„MD5å€¼
        }

        return new Promise((resolve, reject) => {
            const reader = new FileReader()
            reader.readAsArrayBuffer(fileChunkList[index].chunkFile)
            reader.onload = (e) => {
                count++
                spark.append(e.target.result)

                // æ›´æ–°è¿›åº¦å¹¶å¤„ç†ä¸‹ä¸€ä¸ªåˆ‡ç‰‡
                percentage += 100 / fileChunkList.length
                self.postMessage({ percentage }) // å‘é€è¿›åº¦åˆ°ä¸»çº¿ç¨‹

                resolve(loadNext(index + 1)) // é€’å½’è°ƒç”¨ï¼Œå¤„ç†ä¸‹ä¸€ä¸ªåˆ‡ç‰‡
            }
            reader.onerror = (err) => {
                reject(err) // å¦‚æœè¯»å–é”™è¯¯ï¼Œåˆ™æ‹’ç»Promise
            }
        })
    }

    try {
        // å¼€å§‹è®¡ç®—åˆ‡ç‰‡
        const fileHash = await loadNext(0) // ç­‰å¾…æ‰€æœ‰åˆ‡ç‰‡å¤„ç†å®Œæ¯•
        console.log(`æ–‡ä»¶åˆ‡ç‰‡ç»“æœ`, fileHash)

        self.postMessage({ percentage: 100, fileHash, fileChunkList }) // å‘é€æœ€ç»ˆç»“æœåˆ°ä¸»çº¿ç¨‹
        self.close() // å…³é—­Worker
    } catch (err) {
        self.postMessage({ name: 'error', data: err }) // å‘é€é”™è¯¯åˆ°ä¸»çº¿ç¨‹
        self.close() // å…³é—­Worker
    }
}

// ç›‘å¬æ¶ˆæ¯
onmessage = async (e) => {
    try {
        const { file, chunkSize } = e.data
        const fileChunkList = await createFileChunk(file, chunkSize) // åˆ›å»ºæ–‡ä»¶åˆ‡ç‰‡
        if (file.size < fileIncrementalMdSize) {
            console.log(
                `æ–‡ä»¶çš„å¤§å°å°äº${
                    fileIncrementalMdSize / 1024 / 1024
                }MBï¼Œé‡‡ç”¨å…¨é‡è®¡ç®—ï¼Œå¯èƒ½ä¼šå´©æºƒğŸ˜«,å¦‚æœè¿™éƒ½å—ä¸äº†ï¼Œè¿˜çœŸæ˜¯ä¸€ä¸ªæ‚é±¼æµè§ˆå™¨å‘¢â¤ï¸â¤ï¸â¤ï¸`,
            )
            await calculateChunksHashMin(file, fileChunkList)
        } else {
            await calculateChunksHash(fileChunkList) // ç­‰å¾…è®¡ç®—å®Œæˆ
        }
    } catch (err) {
        // è¿™é‡Œå®é™…ä¸Šä¸ä¼šæ•è·åˆ°calculateChunksHashä¸­çš„é”™è¯¯ï¼Œå› ä¸ºé”™è¯¯å·²ç»åœ¨Workerå†…éƒ¨å¤„ç†äº†
        // ä½†å¦‚æœæœªæ¥æœ‰å…¶ä»–çš„å¼‚æ­¥æ“ä½œï¼Œè¿™é‡Œå¯ä»¥æ•è·åˆ°å®ƒä»¬
        console.error('workerç›‘å¬å‘ç”Ÿé”™è¯¯:', err)
    }
}

// ä¸»çº¿ç¨‹å¯ä»¥ç›‘å¬ Worker æ˜¯å¦å‘ç”Ÿé”™è¯¯ã€‚å¦‚æœå‘ç”Ÿé”™è¯¯ï¼ŒWorker ä¼šè§¦å‘ä¸»çº¿ç¨‹çš„erroräº‹ä»¶ã€‚
onerror = (event) => {
    console.log('Workerè§¦å‘ä¸»çº¿ç¨‹çš„erroräº‹ä»¶ï¼š', event)
    self.close() // å…³é—­Worker
}
